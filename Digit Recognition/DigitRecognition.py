# -*- coding: utf-8 -*-
"""patrec_Tsaousis_Kostopoulos.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_SwRJmcF_CUEUNmoAzTYujORx70n6l4k

**Αναγνώριση Προτύπων, 1η εργαστηριακή άσκηση**



*Κωνσταντίνος Τσαούσης 03117652,*
*Κωνσταντίνος Κωστόπουλος 03117043*
"""

!pip install --user sklearn
!pip install --user scikit-image
!pip install --user numpy
!pip install --user matplotlib
!pip install --user pandas
!pip install --user torch
!pip install --user torchvision
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import random
#from random import randrange
from sklearn.base import BaseEstimator
from sklearn.model_selection import cross_validate
from sklearn.decomposition import PCA
import matplotlib.cm as cm
from matplotlib.lines import Line2D
from sklearn.model_selection import learning_curve
import math
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn import svm
from sklearn.metrics import confusion_matrix, f1_score
from sklearn.utils.multiclass import unique_labels
from  matplotlib.colors import LinearSegmentedColormap
from matplotlib.colors import from_levels_and_colors
import seaborn as sns; sns.set()
from mlxtend.classifier import EnsembleVoteClassifier
import torch
from torch.utils.data.sampler import SubsetRandomSampler
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.ensemble import BaggingClassifier

import numpy as np
import matplotlib.pyplot as plt

with open('train.txt', 'r') as f:
    l = [[(num) for num in line.split(' ')] for line in f]
DATA = np.zeros( (len(l), len(l[0])-1) )

for i in range(len(l)-1):
    for j in range(len(l[0])-1):
        DATA[i][j]=float(l[i][j][:])

X_train = DATA[:,1:]
Y_train = DATA[0:,0]
print(X_train.shape)
print(Y_train.shape)

with open('test.txt', 'r') as f:
    l = [[(num) for num in line.split(' ')] for line in f]
DATA = np.zeros( (len(l), len(l[0])-1) )

for i in range(len(l)-1):
    for j in range(len(l[0])-1):
        DATA[i][j]=float(l[i][j][:])

X_test = DATA[:,1:]
Y_test = DATA[0:,0]
print(X_test.shape)
print(Y_test.shape)

"""# Βήμα 2 
Εκτυπώνουμε την 131η εικόνα του train set.
"""

fontdict = {'color' : 'black',
        'weight' : 'bold',
        'size'   : 24}

im131=X_train[131,:]
image131 = np.reshape(im131, (16,16))

plt.imshow(image131, cmap='Greys', vmin=-1, vmax=1)
plt.title((Y_train[131]), y=1.00, **fontdict)
plt.axis("off")

"""# Βήμα 3
Διαλέξτε 1 τυχαίο δείγμα από κάθε label ,συνολικά 10 δείγματα). Σχεδιάστε τα σε ένα figure με subplots.
(Hint: fig = plt.figure(); fig.add_subplot(,,,))
"""

labels = [np.where(Y_train==0)[0],np.where(Y_train==1)[0],np.where(Y_train==2)[0],np.where(Y_train==3)[0],
           np.where(Y_train==4)[0],np.where(Y_train==5)[0],np.where(Y_train==6)[0],np.where(Y_train==7)[0],
           np.where(Y_train==8)[0],np.where(Y_train==9)[0]]

#for each each digit we choose a random array of its features
random_samples = [random.choice(labels[0]),random.choice(labels[1]),random.choice(labels[2]),
                 random.choice(labels[3]),random.choice(labels[4]),random.choice(labels[5]),
                 random.choice(labels[6]),random.choice(labels[7]),random.choice(labels[8]),
                 random.choice(labels[9])]
# plot in 2x5 format
for i in range(2):
    f, (ax0, ax1, ax2, ax3,ax4) = plt.subplots(1, 5, figsize=(20, 20))

    ax0.imshow(X_train[random_samples[i*5]].reshape(16,16), cmap='Greys')
    ax0.set_title(Y_train[random_samples[i*5]], fontsize=14)
    ax1.imshow(X_train[random_samples[i*5+1]].reshape(16,16), cmap='Greys')
    ax1.set_title(Y_train[random_samples[i*5+1]], fontsize=14)
    ax2.imshow(X_train[random_samples[i*5+2]].reshape(16,16), cmap='Greys')
    ax2.set_title(Y_train[random_samples[i*5+2]], fontsize=14)
    ax3.imshow(X_train[random_samples[i*5+3]].reshape(16,16), cmap='Greys')
    ax3.set_title(Y_train[random_samples[i*5+3]], fontsize=14)
    ax4.imshow(X_train[random_samples[i*5+4]].reshape(16,16), cmap='Greys')
    ax4.set_title(Y_train[random_samples[i*5+4]], fontsize=14)

"""# Βήμα 4"""

sum = 0
for i in labels[0]: #take all zeros
    train_zero = X_train[i].reshape(16,16)
    sum += train_zero[10][10] # we sum all the values of pixels [10][10] of zero samples
mean_value = sum/len(labels[0]) #we divide it by the total number of zero samples
print(mean_value)

"""Παρατηρούμε ότι η mean τιμή για το pixel (10,10) είναι πολύ μικρή (Κλίμακα [-1,1] ). Αυτό είναι λογικό και αναμενόμενο δεδομένου του ότι πρόκειται για απεικονίσεις του ψηφίου 0, το οποίο έχει κενό στο κέντρο.

# Βήμα 5
"""

res= X_train[Y_train==0] 

i=10*16+10-1 

std= res[:,i].std()
print(std)

"""placedholder καποιου σχολιου

# Βήμα 6
"""

mean = np.reshape(np.mean(res, axis=0),(16,16))
std = np.reshape(np.std(res, axis=0),(16,16))

var = std**2
print(var[10][10])

"""Σχόλιο

# Βήμα 7
"""

plt.imshow(mean, cmap='Greys', vmin=-1, vmax=1)
plt.axis("off")

"""σχόλιο

# Βήμα 8
"""

plt.imshow(std, cmap='Greys', vmin=-1, vmax=1)
plt.axis("off")

"""Σχόλιο διαφορές.....

# Βήμα 9

### α)
"""

avg = np.zeros((10,16,16))
cnt = np.zeros((10,16,16))

for i in range(10):
  res= X_train[Y_train==i]
  avg[i]=(np.reshape(np.mean(res, axis=0),(16,16)))
  cnt[i]=(np.reshape(np.std(res, axis=0),(16,16)))

"""### β)"""

fig = plt.figure(figsize=(16,8))
for i in range(10):
  res= X_train[Y_train==i] 
  mean = np.reshape(np.mean(res, axis=0),(16,16))
  fig.add_subplot(3,5,i+1)
  plt.imshow(mean, cmap='Greys', vmin=-1, vmax=1)
  plt.axis("off")

"""Σχόλια

# Βήμα 10

Χρησιμοποιώντας την ευκλείδια απόσταση, δηλαδή την νόρμα l2 και παίρνοντας τις εικόνες με τις μέσες τιμές των pixels του training set ως κέντρα, ταξινομώ την 101η εικόνα με το label που ελαχιστοποιεί αυτή την νόρμα. Στην προκειμένη περίπτωση θέλουμε να είναι το 0.
"""

sample=X_test[100,:]
dist=[]
for i in avg:
  dist.append(np.linalg.norm(np.reshape(i, 256) - sample))

print("Prediction: The digit", np.argmin(dist))
print("True label: The digit", Y_test[100])

"""Παρατηρούμε ότι η πρόβλεψη είναι σωστή.

# Βήμα 11
Κάνοντας το ίδιο για κάθε εικόνα του test set έχουμε το παρακάτω accuracy:
"""

classify=[]
for test in X_test:
    dist=[]
    for label in avg:
        dist.append(np.linalg.norm(np.reshape(label, 256)-test))
    classify.append(np.argmin(dist))
    
hits = sum(classify == Y_test) 
total = len(Y_test)
print("accuracy", hits / total)

print('label', classify[100])
print("real", int(Y_test[100]))

"""Έχουμε δηλαδή ποσοστό επιτυχίας 81.4% επι του συνολικού test set

# Βήμα 12
"""

from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.utils.validation import check_X_y, check_array, check_is_fitted
from sklearn.utils.multiclass import unique_labels
from sklearn.model_selection import learning_curve
import numpy as np
class EuclideanDistanceClassifier(BaseEstimator, ClassifierMixin):
    """Classify samples based on the distance from the mean feature value"""

    def __init__(self):
        self.X_mean_ = None
        self.classes = None

    def toNp(self, l):
      if type(l) == list:
        return np.array(l)
      return l    



    def fit(self, X, y):
        """
        This should fit classifier. All the "work" should be done here.
        Calculates self.X_mean_ based on the mean
        feature values in X for each class.
        self.X_mean_ becomes a numpy.ndarray of shape
        (n_classes, n_features)
        fit always returns self.
        """

        X=self.toNp(X)
        Y=self.toNp(y)
        # assign classes
        self.classes= unique_labels(Y)
        (_, n_features)= X.shape
        # initialize
        # print(self.classes)
        self.X_mean_= np.zeros(( (len(self.classes)), n_features))
        for i in self.classes:

         label=int(i) 
         sample = X[y==label]
         mean= np.mean(sample, axis=0)
         self.X_mean_[label]=mean

        return self


    def predict(self, X):
        """
        Make predictions for X based on the
        euclidean distance from self.X_mean_
        """
        predictions=[]
        for sample in X:
          dist=[]
          for i in self.X_mean_:
            dist.append(np.linalg.norm(i - sample))
          predictions.append(np.argmin(dist))
        return predictions



    def score(self, X, y):
        """
        Return accuracy score on the predictions
        for X based on ground truth y
        """
        predictions=self.predict(X)
        correct= sum(predictions==Y_test)
        all=len(predictions)
  
        return (correct/all)

classifier= EuclideanDistanceClassifier()
_= classifier.fit(X_train,Y_train)
score= classifier.score(X_test,Y_test)
print('Euclidean Classifier accuracy:',round(score*100,2), "%")
#

"""# Step 13

### α)
"""

#In real world applications we do not have access to test data
#so, only train set is being consider to evaluate the performance of the classifier
resutls= cross_validate(classifier, X_train, y=Y_train, cv=5, return_train_score=True, scoring=['accuracy'])
print(resutls)
print("average:", np.mean(score))

"""### β) 

In order to plot the decision region of the Euclidean Classifier on an image (2D) we have to reduce the 256 dimensions (pixels) into 2. In order to do this, we will apply principal component analysis (PCA) 
"""

#we will use pca with 2 principal components
pca= PCA(n_components=2)
X_train_PCA= pca.fit_transform(X_train)

clf_PCA= EuclideanDistanceClassifier()
_= clf_PCA.fit(X_train_PCA,Y_train)

def plot_clf(clf, X, y,):
    fig, ax = plt.subplots()

    # grid.
    X0, X1 = X[:, 0], X[:, 1]
    
    x_min, x_max = X0.min() - 0.5, X0.max() + 0.5
    y_min, y_max = X1.min() - 0.5, X1.max() + 0.5
    xx, yy = np.meshgrid(np.arange(x_min, x_max, .05),
                         np.arange(y_min, y_max, .05))
    
    Z = np.array(clf.predict(np.c_[xx.ravel(), yy.ravel()]))
    Z = Z.reshape(xx.shape)
    
    cmap = cm.get_cmap("jet",10)
    custom_lines =[ Line2D([0], [0], color=cmap(i), lw=7) for i in range(11)]
    
    out = ax.contourf(xx, yy, Z+0.5, alpha=0.8, cmap=cmap, levels = np.append(np.unique(y),max(y)+1))
        
    ax.scatter(X0, X1, c=y, s=60, alpha=0.8, cmap=cmap,edgecolors='k')
        
        for cmean,cclass in zip(clf.X_mean_,clf.classes):
            plt.text(cmean[0],cmean[1], str(cclass), fontsize=12)
        plt.text(np.min(xx), np.min(yy)-1, '*numbers are placed at means\' position', fontsize=12)
    
    ax.set_xticks(())
    ax.set_yticks(())
    ax.set_title(title)
    ax.legend(custom_lines, list(range(10)))
    plt.show()

labels= ['digit'+str(digit) for digit in range(10)]

plot_clf(clf_PCA, X_train_PCA, Y_train)

"""13γ)"""

def plot_learning_curve(train_scores, test_scores, train_sizes, ylim=(0, 1)):
    plt.figure()
    plt.title("Learning Curve")
    if ylim is not None:
        plt.ylim(*ylim)
    plt.xlabel("Training examples")
    plt.ylabel("Score")

    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)
    plt.grid()

    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                     train_scores_mean + train_scores_std, alpha=0.1,
                     color="r")
    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
                     test_scores_mean + test_scores_std, alpha=0.1, color="g")
    plt.plot(train_sizes, train_scores_mean, 'o-', color="r",
             label="Training score")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="g",
             label="Cross-validation score")

    plt.legend(loc="best")
    # return plt

plot_learning_curve(train_scores, test_scores, len(X_train), ylim=(.6, 1))

"""# Βήμα 14"""

# Find the a priori probalities for each class/label 
labels = [([Y_train == label]) for label in unique_labels(Y_train)]
Y_train_size = len(Y_train)

a_priori = [(np.sum(label, axis=1) / Y_train_size)[0] for label in labels]
print(a_priori)

plt.figure(figsize=(9, 3))
plt.subplot()
plt.bar([0,1,2,3,4,5,6,7,8,9], a_priori)
# Checked sum(a_priori = 1)

print(sum(a_priori))

"""# Βήμα 15

### α)

![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOIAAAAsCAYAAAB48mQTAAAJjklEQVR4Ae2cPa4EtRKFSyJABEiEEEFMBBkZbACBEDkIFsCLyBDoRS+DlAgyQtgB7ABWAOwAdgD69OagM8Z2e7rt7p65LqnVPf4pl6vqVJV75t6ISVMDfTTwYUR81YfVXXL5IiJ+iojfIuK1ZAd8fsq6SdQxP47SACD8dhTzO+D7bkS8cpHzPxHxQ0bmtyLim0z7bJoa6KIBoj2ZYNL/NQDgSkEJkJI5J00NdNXACxHxi2WDXszJMPA+KyFbWn5KVsBW6mMMQQuwTpoa6KYBzj29IzxOTKl7dgJwKkclK7ITRGrEHM6RZw40Nfln38k0gEP9OcCh7uWlBkByWVMQ1oBG6do7gJ3MPaY4e2lghDOlzq29vGnZh+cRRGAReACVnmtr6aWMstyPEcG1dGZm/B+Na9TWn30dNMA5YamM6bDMEBY40l8Gjl6LoJNcpkBPAJ/7KJ0BHvYFtZaOyLr2vMd6lLeTDtQA0XZERtlrSziQskHPNXHqknPmMgjAyWWu/10ykzKU7p9WhP390kc2XMpoYrMFiJyDAfykgzWAEXPR/2Cxmpb/uQKYJgaFQaWMCOBy51GcuUeGZF0CI5QGSPjLTqwHUEVbgEgA6VZVlCKSBN3zrrJizzW3rHWvQETPOJA75BY9+Fx4yulpx1lZDzDwYoT7CDuzJvwBGpk+XYevaCSL75s5W+SBb6kCcL1UnxHI3xpVBzd0vhQRrzaMKw1Jo1Vp3Fna7xWIOClAHEXKTPB3H8Ph0z6Vk1tloRTFlxUI8CUnbMW+03aXx8e3PrNmtsRHEDqoqRnExc9ycgfS1jq6VaiPI+K71sGFcci+JUIV2A5pvlcgIndv27uCyRAtNpSv+ty1z0v7wf8FVK3B+ikw1dd6Z6/FcyJCUY+LGEwEJCKIFCH0uce9BxC91u8h00ge9wpE/GNrJljSK7pZIvySayvhMy2ASjNXy5wl2Vi7WF2AUF9Uh0pXDrVtb+oBRGRSPd9bvp780Cn6JMryfE/Eixr3hRGyo5MlveCjZKU9CMC4PP68ZX3kB4jwvyJ1ONopExis6MOYWiqn/x3jyiItpcYSEFO+LOHraEmitcuv9qd0x1E+vxwrPrGNU9XQji0Zg554ln240w8xj2PJB5fPuuELo4GotWp3ZPiXA9cmrOiDf1qSrmBTnZIFIkahww1DBPQMiAJKhmCeDvM8w4+NeFlbkqoGRJwGHpTMAhn8+QxAnZCNNUuEoy1dow1ckq1HO7oiW3CHCJqc+UXYUp/dyRiP7gQ09O2fNZ/+Fntq/LzXNYA+5dP/jNT5EANhTO7pIIzDVSOMTTRNQVKbUwOi5kkufQboKQGiWsaW/LV7DYgvXiIxY85yvWxKYF9fRgQ/BePCDh5csQmfGZPqj/3QJxDDlgqDL9NF9Nf0o3Fr7mfRZw853mhUAPq8whPKpxFnr5EcuDYG45UO9GQyrpRagCgn0tw0SNCOEmtA1Ny1948uekJXZ7k+s82wdwCWOpODCxsiu7dJd7Q7kf28jWd4t5CCAHNK13sXRs9UxpTmnrn91xYFXfZ8BUQpfEnJrUCkpM0R4MmVNi1AhB/lqJwsB2j6akBUpqjdc3y1l7NnRPZ+ZVgJbnf6+Q7ue2vjEd3h3E5pG/20jSD4Psq1OiOSwfxri5Kil4AIyAAbBiPiplG3xLcViJTMyJCWVeJLey2r46hLVy7Tin/r/bmIeH6n61kTCv2kP1qmklDww9GVMbGR2mFBXwpExsJTNBKIWuMp3dHnlS9jvFomkXIwam4cb9d4iynDAuoUtMwt/RqiFYiK5iWA5861kn3P+9cR8d+drrdtY2RzjEtFwptPXkwJSOhMz0xBV5z/VAEIiApEjOe8j91E8MYGj0TskzKal1icnfekq8CG4gUaV3pJoByYMLAjG55kWQcMvN0RnH8rEAG6wO7z9YwDyrHUdsTddbH3+ugZkBEwkQMbcGETLtlEdlebgIh+mY9PpP4AMB8NiNo/ekFnNf/qaUt0ewXEW5ljiDXCMo9N56gFiCiq5uAAsAT03Jqj2pCjtM9Ra/bgKyDWeOGogPSRSIGJPa317TX6kL59/Zv4MHGNw2PENMJq4RoQcWqcewn8ZOAzZENFWO1txL2kxy1ryTFqOlSm3bLO6LkqydfoqPS2f4TM+EnLe5nq2myylp1ykylpS3Nej4j3c5MuJe5SOcSmuM5AS7JukRGwCAyU4aujaSIEfPR1A+ekEhjRsX+vmLA5/CP+RcBGTsq+0j5ygjJvDXhzvFraZMeWsdUxCN3LEaoLNXTioGehUrDpIZ9XBZzX9t43jn2rg/fYdysP1wdVm+urxgNfFgj38mkC6cigXdvvw/fhCDLo6M3iaLdE/F7y1CqbXmv04MNRyPUDwChbeTvKpTIUe1EN6KePe5yBkYWAtpev9NDnXfFIIxwRWYZvjc4tG8aQ6Vot83qMwVHXvCPosXYrDwDo+kFfZCCqFZWv4gUYCKC6HLwa0/uOL+S+fei9zpPl52UpBtW5FcX3PFuJ7xGKZl9Ec5z7jIRcbgdkBJTKdPRT1h9JBDIPFEfK8nBrO/DYXBpZcd4e5CA8qrTBkVJn77G3rTxSEMoGlKk6PxIUBUrWYwygkF5HZ3tkPHMg22qDw+djSJScI9p1JqGfzzgD5QnRmbs7B86idv/uTm82MSSXnCe35sg2nLr4bx5GLrzAW8cA3aVTggZnQOSmLbUTYBQA0f1I8uw8cp0nwVuR1jdbKzXoc+PzjFMAJJzEMxvtgJAxOMVRYPO95Z4JLGeVLSev6z/Xr3IV/Y8iZCDoLskyav2H4guocqDLtbFxwORAkzJwZABNxnPis6IyTpH2exDwZ+exxzNrP5JTkSmVQUfpj794uafgNUoPm/nifICDN3BOtOUiKUrnlbn+7CoFGPzgpXZ4AjwBDLB6H+2eiXg+kpAt/XOqI+XZsjaVSc6GW3j6XHR1tL1cnod45qdJXl7ksiFZEFD5JYBRBtEPD/rdATAYTsHd26U4eOg8k+vXuL3uBBvkvXcaqUv5gvvMvevrFPIDBIAiygFRfSPuArKAPWKNW3gCxnstubDdSNknCG/xpBvHkgF0ngAMIw2ZEy0tWXNj9m47S1DYe99L66GXmQmXtLSynyincyIg3NsJWXMad6Xx5rTH0oDeGO5ZlgJ4LgLBpKmBqYHLGzDOiXsCkZJ4gnC639SAaYDykPJ05Ns2W24+Tg1MDeQ0QIm4+S+sc4xn29TA1MBtGij9V4HbuMzRT1oDfwMO/F+fO7TjYwAAAABJRU5ErkJggg==)

Λόγω του ότι η διασπορά σ ειναι στον παρανομαστή, μηδενικές τιμές της θα μας προκαλέσουν πρόβλημα. Για αυτό θα προσθέσουμε ένα μικρό offset
"""

class CustomNBClassifier(BaseEstimator, ClassifierMixin):
    """Custom implementation Naive Bayes classifier"""

    def __init__(self, use_unit_variance=False, std0=False):
        self.std0=std0
        self.use_unit_variance = use_unit_variance
        self.avg = None
        self.classes = None
        self.stds=None
        self.apriori=None
        


    def toNp(self, l):
      if type(l) == list:
        return np.array(l)
      return l    


    def fit(self, X, y):
        """
        This should fit classifier. All the "work" should be done here.
        Calculates self.X_mean_ based on the mean
        feature values in X for each class.
        self.X_mean_ becomes a numpy.ndarray of shape
        (n_classes, n_features)
        fit always returns self.
        """
        #a priori probabilites (if not given by the user)

        if self.apriori==None:
          self.classes= unique_labels(y)
          labels = [([y == label]) for label in self.classes]
          total = len(y)
          self.apriori = [(np.sum(label, axis=1) / total)[0] for label in labels]



        ## find mean
        self.avg= np.array([ X[y == digit].mean(axis=0) for digit in self.classes])

        ## find std, if not already given
        if not self.std0:
          self.stds= np.array([ X[y == digit].std(axis=0) for digit in self.classes])
        else: ## std given so we make the np array from scratch from the given value
          features=self.avg.shape 
          self.stds= self.std0*np.ones(features)

        ### add an offset to avoid having 0 values of stds.
        # minFstd=self.stds(self.stds>0).min()
        self.stds+=1/10000000


        return self


    def predict(self, X):
        """
        Make predictions for X based on the
        euclidean distance from self.X_mean_
        """
        ###predict, we use log to have a better arithmetic stability, using + operator and not *
        prediction=np.zeros(len(X))
        apriorilog=np.log(self.apriori)
        i=0 ##init
        for sample in X:


          summean=np.sum(((sample-self.avg)**2)/((self.stds**2)), axis=1)
          sumstd=np.sum(np.log(1/(np.sqrt(2*math.pi*(self.stds**2)))), axis=1)

          res= sumstd-summean+ apriorilog

          prediction[i]=np.argmax(res) ##get the correct label
          i+=1

        return prediction




    def score(self, X, y):
        """
        Return accuracy score on the predictions
        for X based on ground truth y
        """

        predictions=self.predict(X)
        correct= sum(predictions==Y_test)
        all=len(predictions)

        return (correct/all)



# avg = np.zeros((10,16,16))
# cnt = np.zeros((10,16,16))

# for i in range(10):
#   res= X_train[Y_train==i]
#   avg[i]=np.mean(res, axis=0)
#   cnt[i]=np.std(res, axis=0)

"""### β) Predictions, accuracy and crossvalidation accuracy"""

cnbc=CustomNBClassifier()
cnbc.fit(X_train, Y_train)
y_predict=cnbc.predict(X_test)
print(y_predict)

###μάλλον θα το σβήσουμε


cnbc= CustomNBClassifier()
cnbc.fit(X_train, Y_train)
print('Accuracy: {:.3f} '.format(cnbc.score(X_test,Y_test)))

resutls= cross_validate(cnbc, X_train, y=Y_train, cv=5, return_train_score=True, scoring=['accuracy'])
print(resutls)
print("average:", np.mean(np.array(resutls["test_accuracy"])))

"""Observing the bayesian type we see that it is a generalization of the mean euclidean, taking into account distributions and a priori probabilities. If the distributions and a priori probabilities are the same for all attributes and classes, then the two classifiers work the same way. The deterioration is probably due to the assumption of the pixel distributions that we made and ultimately do not apply (independent normal).

### γ) compare with Sklearn Naive Bayes
"""

sknb=GaussianNB()
resutls= cross_validate(sknb, X_train, y=Y_train, cv=5, return_train_score=True, scoring=['accuracy'])
print(resutls)
print("average:", np.mean(np.array(resutls["test_accuracy"])))

sknb.fit(X_train,Y_train)
sknb.score(X_test,Y_test)

"""To get a better insight on the data and the classification norms, we will use the fine grained metric of the confusion matrix, which shows the number of samples that get missclassified in each class.

Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa).
"""

def plot_confusion_matrix(y_true, y_pred, fig, ax, allClasses, 
                          normalize=False,
                          title=None,
                          cmap=plt.cm.OrRd):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if not title:
        if normalize:
            title = 'Normalized confusion matrix'
        else:
            title = 'Confusion matrix, without normalization'

    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    # Only use the labels that appear in the data
    classes = allClasses[(unique_labels(np.array(y_true).astype(int), np.array(y_pred).astype(int)))]
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]


    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)
    ax.figure.colorbar(im, ax=ax)
    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           # ... and label them with the respective list entries
           xticklabels=classes, yticklabels=classes,
           title=title,
           ylabel='True label',
           xlabel='Predicted label')

    # Rotate the tick labels and set their alignment.
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
             rotation_mode="anchor")

    # Loop over data dimensions and create text annotations.
    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], fmt),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black")
    fig.tight_layout()
    return ax

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))

classes=Y_test, sknb.predict(X_test)

# classes = first[(unique_labels(Y_test.astype(int), (sknb.predict(X_test)).astype(int)))]
# print('Comparison of Confusion matrices of Gaussian Naive Bayes Models')

# Plot non-normalized confusion matrix
plot_confusion_matrix(Y_test, sknb.predict(X_test), fig, ax1, allClasses=np.arange(10), title='Scikit Learn Naive Bayes');
#plt.subplot(1,2,ax1)


# Plot non-normalized confusion matrix
plot_confusion_matrix(Y_test, cnbc.predict(X_test), fig, ax2, allClasses=np.arange(10), title='Our Naive Bayes');


#plt.subplot(1,2,ax2)

"""Above we see the SK learn implemented NB and our one implemented from scratch. They have a very similar pattern of results and mispredictions, with both of them often confusing the number 4 for the 9 (117 misspredictions for the SKlearn and 106 for ours), the 3 for the 8 and the 0 for the 8 as well. Our one is behaving a little bit better, which is probably caused by the slightly different implementations, perhaps on how we treated the 0 stds.

# Βήμα 16 

### Naive Bayes Gaussis with variance=1
"""

onenb=CustomNBClassifier(std0=1)
resutls= cross_validate(onenb, X_train, y=Y_train, cv=5, return_train_score=True, scoring=['accuracy'])
print(resutls)
print("average:", np.mean(np.array(resutls["test_accuracy"])))

onenb.fit(X_train,Y_train)
onenb.score(X_test,Y_test)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))

classes=Y_test, sknb.predict(X_test)

# classes = first[(unique_labels(Y_test.astype(int), (sknb.predict(X_test)).astype(int)))]
# print('Comparison of Confusion matrices of Gaussian Naive Bayes Models')

# Plot non-normalized confusion matrix
plot_confusion_matrix(Y_test, cnbc.predict(X_test), fig, ax1, allClasses=np.arange(10), title='Extracted stds Naive Bayes');
#plt.subplot(1,2,ax1)


# Plot non-normalized confusion matrix
plot_confusion_matrix(Y_test, onenb.predict(X_test), fig, ax2, allClasses=np.arange(10), title='Stds=1 Naive Bayes');
#plt.subplot(1,2,ax2)

"""*With a standard deviation equal to 1 the classifier works pretty much like the Euclidean Mean Distance classifier, with the only difference being the subtraction of the logarithm of the a prioris*

The classifier works way better with an std=1, making the misspredictions of the digit 4 for 9 and 3 for 8 much fewer.

# Βήμα 17

### K-Nearest Neighbors Classification Algorithm (k-NN) : each new sample is classified by the majority vote of its k neighbors.

Experimenting with different values of n, we observe a decrease in accuracy as we get to bigger numbers. This means that the score is better with higher variance (closer to 1) , while we can conclude that the train and test set have similar distributions.
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

###Accuracy on the test dataset

knn1 = KNeighborsClassifier(n_neighbors=1)
knn1.fit(X_train, Y_train)
y_predict = knn1.predict(X_test)
print("KNN Score ~ n=1 :", accuracy_score(Y_test, y_predict))

knn2 = KNeighborsClassifier(n_neighbors=10)
knn2.fit(X_train, Y_train)
y_predict = knn2.predict(X_test)
print("KNN Score ~ n=10 :", accuracy_score(Y_test, y_predict))

knn3 = KNeighborsClassifier(n_neighbors=50)
knn3.fit(X_train, Y_train)
y_predict = knn3.predict(X_test)
print("KNN Score ~ n=50 :", accuracy_score(Y_test, y_predict))

knn4 = KNeighborsClassifier(n_neighbors=100)
knn4.fit(X_train, Y_train)
y_predict = knn4.predict(X_test)
print("KNN Score ~ n=100 :", accuracy_score(Y_test, y_predict))


### accuracy on the cross validation of the train dataset
resutls= cross_validate(knn1, X_train, y=Y_train, cv=5, return_train_score=True, scoring=['accuracy'])
# print(resutls)
print("average cv accuracy for n=1 :", np.mean(np.array(resutls["test_accuracy"])))

resutls= cross_validate(knn2, X_train, y=Y_train, cv=5, return_train_score=True, scoring=['accuracy'])
# print(resutls)
print("average cv accuracy for n=10 :", np.mean(np.array(resutls["test_accuracy"])))

resutls= cross_validate(knn3, X_train, y=Y_train, cv=5, return_train_score=True, scoring=['accuracy'])
# print(resutls)
print("average cv accuracy for n=50 :", np.mean(np.array(resutls["test_accuracy"])))

resutls= cross_validate(knn4, X_train, y=Y_train, cv=5, return_train_score=True, scoring=['accuracy'])
# print(resutls)
print("average cv accuracy for n=100 :", np.mean(np.array(resutls["test_accuracy"])))

"""### Support Vector Machine (SVM)"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
from sklearn.svm import SVC

parameters = [{'kernel': ['linear'], 'C': [1, 10, 100]},
                    {'kernel': ['poly'], 'C': [1, 10, 100, 1000], 'degree':[3,5,7,9]}
                    ,
                    {'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],
                     'C': [1, 10, 100, 1000]},
                   {'kernel': ['sigmoid'], 'C': [1, 10, 100, 1000]}
                    ]



# GridSearchCV for tuning the parameters using f1 weighted scoring (weighted average of Precision and Recall)
clf = GridSearchCV(SVC(), parameters, cv=5,
                       scoring="f1_weighted")
clf.fit(X_train, Y_train)

print("Optimal parameters found on train set:", clf.best_params_)
means = clf.cv_results_['mean_test_score']
stds = clf.cv_results_['std_test_score']
for mean, std, params in zip(means, stds, clf.cv_results_['params']):
    print("%0.3f (std %0.03f) for %r"
#           % (mean, std * 2, params))
y_true, y_predict = Y_test, clf.predict(X_test)
print(classification_report(y_true, y_predict))
print()

parameters= [ [CustomNBClassifier(), 'Custom Naive Bayes '], 
              [GaussianNB(), 'Gaussian Naive Bayes (ScikitLearn)']
             [KNeighborsClassifier(n_neighbors=1)(), "KNN with k=1"],
             [EuclideanDistanceClassifier(), " EuclideanDistanceClassifier"],
             []

             

]

"""# Βήμα 18

Before combining our classifiers to get the best results, we have to analyse better our outcomes and chose which factor needs improvement, like which class gets usually misspredicted etc.

To get a better insight on the data and the classification norms, we will use the fine grained metric of the confusion matrix, which shows the number of samples that get missclassified in each class.

Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa).
"""

sknb=EuclideanDistanceClassifier()

from sklearn.metrics import confusion_matrix
sknb.fit(X_train, Y_train)
cm = confusion_matrix(Y_test, sknb.predict(X_test))


print(cm)

def plot_confusion_matrix(y_true, y_pred, fig, ax, allClasses, 
                          normalize=False,
                          title=None,
                          cmap=plt.cm.OrRd):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if not title:
        if normalize:
            title = 'Normalized confusion matrix'
        else:
            title = 'Confusion matrix, without normalization'

    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    # Only use the labels that appear in the data
    classes = allClasses[(unique_labels(np.array(y_true).astype(int),np.array( y_pred).astype(int)))]
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]


    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)
    ax.figure.colorbar(im, ax=ax)
    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           # ... and label them with the respective list entries
           xticklabels=classes, yticklabels=classes,
           title=title,
           ylabel='True label',
           xlabel='Predicted label')

    # Rotate the tick labels and set their alignment.
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
             rotation_mode="anchor")

    # Loop over data dimensions and create text annotations.
    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], fmt),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black")
    fig.tight_layout()
    return ax

fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,5))

classes=Y_test, sknb.predict(X_test)


# classes = first[(unique_labels(Y_test.astype(int), (sknb.predict(X_test)).astype(int)))]
# print('Comparison of Confusion matrices of Gaussian Naive Bayes Models')

# Plot non-normalized confusion matrix
plot_confusion_matrix(Y_test, sknb.predict(X_test), fig, ax1, allClasses=np.arange(10), title='Scikit Learn Euclidean');
# plt.subplot(1,2,ax1)


# Plot non-normalized confusion matrix
plot_confusion_matrix(Y_test, cnbc.predict(X_test), fig, ax2, allClasses=np.arange(10), title='Our Naive Bayes');

plot_confusion_matrix(Y_test, classifier.predict(X_test), fig, ax3, allClasses=np.arange(10), title='Euclidean');
# plt.subplot(1,2,ax2)

""" Additionally, we are going to make use of heatmaps to have a better idea about our f1 score for each digit"""

clfs = [
        [svm.SVC(kernel='poly',gamma='scale'), 'SVM w/ polynomial kernel'],
        [svm.SVC(kernel='rbf',gamma='scale'), 'SVM w/ rbf kernel'],
        [KNeighborsClassifier(n_neighbors=3), 'k-Nearest Neighbors Classifier'],
        [svm.SVC(kernel='linear',gamma='scale'), 'SVM w/ linear kernel'],
       ]

f1_res= []
labels= []
for clf, label in clfs:
    f1_res.append(f1_score(Y_test, clf.fit(X_train,Y_train).predict(X_test), average=None))
    labels.append(label)

data= f1_res
plt.figure(figsize=(20,5))
ax = sns.heatmap(np.transpose(data), cmap="RdBu")
ax.set_xlabel("Classifiers")
ax.set_xticklabels(labels)
ax.set_ylabel("Classes")
ax.set_title("HeatMap F1-Score");
plt.show()

fig, axs = plt.subplots(2, 3, figsize=(20,10))

# 

clfs = [
        [svm.SVC(kernel='poly',gamma='scale'), 'SVM w/ polynomial kernel'],
        [svm.SVC(kernel='rbf',gamma='scale'), 'SVM w/ rbf kernel'],
        [KNeighborsClassifier(n_neighbors=3), 'k-Nearest Neighbors Classifier'],
        [svm.SVC(kernel='linear',gamma='scale'), 'SVM w/ linear kernel'],
       ]


poly=svm.SVC(kernel='poly',gamma='scale')
poly.fit(X_train,Y_train)
rbf=svm.SVC(kernel='rbf',gamma='scale')
rbf.fit(X_train,Y_train)
Kn3=KNeighborsClassifier(n_neighbors=3)
Kn3.fit(X_train,Y_train)
linear=svm.SVC(kernel='linear',gamma='scale')
linear.fit(X_train,Y_train)




plot_confusion_matrix(Y_test, cnbc.predict(X_test), fig, axs[0,0], allClasses=np.arange(10), title='Our Naive Bayes');

plot_confusion_matrix(Y_test, classifier.predict(X_test), fig, axs[0,1], allClasses=np.arange(10), title='Euclidean');



plot_confusion_matrix(Y_test, poly.predict(X_test), fig, axs[0,2], allClasses=np.arange(10), title= 'SVM w/ polynomial kernel');

plot_confusion_matrix(Y_test, rbf.predict(X_test), fig, axs[1,0], allClasses=np.arange(10), title='SVM w/ rbf kernel');

plot_confusion_matrix(Y_test, Kn3.predict(X_test), fig, axs[1,1], allClasses=np.arange(10), title='k-Nearest Neighbors Classifier');

plot_confusion_matrix(Y_test, linear.predict(X_test), fig, axs[1,2], allClasses=np.arange(10), title= 'SVM w/ linear kernel');

"""#Now we have a better idea about our classifiers and which classes they are mostly misspredicting. In this step, we are going to use this knowledge and implement

### a)

Hard voting
"""

from sklearn.metrics import accuracy_score
from sklearn.ensemble import VotingClassifier

poly1=svm.SVC(C=10,kernel='poly',probability=True)
poly1.fit(X_train,Y_train)
y_pre1 = poly1.predict(X_test)
acc_score1 = accuracy_score(Y_test, y_pre1)
print("poly1 accuracy: " + str(acc_score1))

                            
poly2=svm.SVC(C=100,kernel='poly',probability=True)
poly2.fit(X_train,Y_train)
y_pre2 = poly2.predict(X_test)
acc_score2 = accuracy_score(Y_test, y_pre2)
print("poly2 accuracy: " + str(acc_score2))


rbf=svm.SVC(C=100,kernel='rbf',gamma=0.001,probability=True)
rbf.fit(X_train,Y_train)
y_pre3 = rbf.predict(X_test)
acc_score3 = accuracy_score(Y_test, y_pre3)
print("rbf accuracy score: " + str(acc_score3))

Kn3=KNeighborsClassifier(n_neighbors=3)
Kn3.fit(X_train,Y_train)
y_pre4 = Kn3.predict(X_test)
acc_score4 = accuracy_score(Y_test, y_pre4)
print("Kn3 accuracy: " + str(acc_score4))

Kn1=KNeighborsClassifier(n_neighbors=1)
Kn1.fit(X_train,Y_train)
y_pre5 = Kn1.predict(X_test)
acc_score5 = accuracy_score(Y_test, y_pre5)
print("Kn1 accuracy: " + str(acc_score5))

"""### Hard Voting"""

from sklearn.ensemble import VotingClassifier

main = VotingClassifier(estimators=[
    ('poly1', poly1),('poly2', poly2), ('Kn1', Kn1), ('Kn3', Kn3), ('rbf', rbf)], 
        voting='hard')
main = main.fit(X_train, Y_train)

main_predict=main.predict(X_test)

print('Hard Voting Score:', accuracy_score(Y_test, main_predict) )

"""### soft voting 

We will try to 
"""

main = VotingClassifier(estimators=[
    ('poly1', poly1),('poly2', poly2), ('Kn1', Kn1), ('Kn3', Kn3), ('rbf', rbf)], 
        voting='soft')
main = main.fit(X_train, Y_train)

main_predict=main.predict(X_test)

print('Soft Voting Score:', accuracy_score(Y_test, main_predict) )


nb=GaussianNB()

main = VotingClassifier(estimators=[
    ('poly1', poly1),('poly2', poly2), ('Kn1', Kn1), ('Kn3', Kn3), ('rbf', rbf),("nb", nb)
], 
        voting='soft')
main = main.fit(X_train, Y_train)

main_predict=main.predict(X_test)


print('Soft Voting Score with Bayes:', accuracy_score(Y_test, main_predict) )

"""###b) Bagging Classifier"""

from sklearn.ensemble import BaggingClassifier 

bagging = BaggingClassifier(base_estimator = KNeighborsClassifier(n_neighbors=3), 
                          n_estimators = 7)
bagging.fit(X_train, Y_train)
y_predict_train = bagging.predict(X_train)
print("Train Bagging KNN =3 score: " + str(accuracy_score(Y_train, y_predict_train)))
y_predict = bagging.predict(X_test)
print("Test Bagging KNN=3  score: " + str(accuracy_score(Y_test, y_predict)))

from sklearn.ensemble import BaggingClassifier 
from sklearn.svm import SVC

bagging = BaggingClassifier(base_estimator = SVC(C=10, kernel='poly', probability=True), 
                          n_estimators = 2, 
)
bagging.fit(X_train, Y_train)
y_predict_train = bagging.predict(X_train)
print("Train Bagging POLY SVM score: " + str(accuracy_score(Y_train, y_predict_train)))
y_predict = bagging.predict(X_test)
print("Test Bagging POLY SVM score: " + str(accuracy_score(Y_test, y_predict)))

from sklearn.ensemble import BaggingClassifier 
from sklearn.tree import DecisionTreeClassifier 

# BaggingClassifier -- Random Forest 
bagging = BaggingClassifier(base_estimator = DecisionTreeClassifier(), 
                          n_estimators = 7, 
                          random_state = 32) 

bagging.fit(X_train, Y_train)
y_predict_train = bagging.predict(X_train)
print("Train Bagging DecisionTreeClassifier score: " + str(accuracy_score(Y_train, y_predict_train)))
y_predict = bagging.predict(X_test)
print("Test Bagging DecisionTreeClassifier score: " + str(accuracy_score(Y_test, y_predict)))

"""Βήμα 18(γ) σχόλια
Βλέπουμε ότι ο μόνο Decision Tree ταξινομητής βελτιώνεται (περιπού κατά 4 ποσοστιαίες μονάδες) με χρήση του bagging, γεγονός που οφείλεται στο ότι είναι αρκετά πιο ασταθής σε σχέση με τους υπόλοιπους (SVM, knn), με αποτέλεσμα στον Random Forest να είναι μειώμενο το Variance σε σχέση με τον Decision Tree.
"""



"""# Βήμα 19

### α)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

def readtxt(txt):
  with open(txt, 'r') as f:
      l = [[(num) for num in line.split(' ')] for line in f]
  d = np.zeros( (len(l), len(l[0])-1) )

  for i in range(len(l)-1):
      for j in range(len(l[0])-1):
          d[i][j]=float(l[i][j][:])

  return d



from torch.utils.data import Dataset,DataLoader

class DigitData(Dataset):
    def __init__(self, txt, trans=None):
        # all the available data are stored in a list
        self.data = readtxt(txt)
        # we optionally may add a transformation on top of the given data
        # this is called augmentation in realistic setups
        self.trans = trans
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()

        sample = np.array(self.data[idx])

        if self.trans:
            sample = self.trans(sample)

        return sample[1:], sample[0]

Train_dataset = DigitData("train.txt")
Test_dataset = DigitData("test.txt")

traindata = DataLoader(Train_dataset, batch_size=100,
                    shuffle=True, num_workers=2)
testdata = DataLoader(Test_dataset, batch_size=100,
                    shuffle=True, num_workers=2)

"""### β)

The model goes as follows. Firstly we flatten our 16*16 image into a 256 element vector (or 256 features). Next we pass the 256 input through the first hidden layer to transform it into 180 dimensions, then the second hidden layer transform it to 100 dimensions and the third to 50. Finally, the output layer which will transform it into a 10 dimensional vector, so the number of classes of our data (0-9 digits).

The transformation between the layers are done by Linear layers, or fully connected/affine layers. In these layers every element in one layer is connected to every element in the next.

Each connection between a neuron in one layer and a neuron in the next has a weight associated with it. The input to one neuron is sum of the weighted values of all neurons in the previous layer connected to it, plus a weighted bias term, where the bias value is always 1. The neuron then applies an activation function to this weighted sum. This activation function is a non-linear function that allows the neural network to learn non-linear functions between inputs and outputs.

We define our MLP below, which consists of three linear layers. We first take the input batch of images and flatten them so they can be passed into the linear layers. We then pass them through the first linear layer, input_fc, which calculates the weighted sum of the inputs, and then apply the ReLU (rectified linear unit) activation function elementwise. This result is then passed through another linear layer, hidden_fc, again applying the same activation function elementwise. Finally, we pass this through the final linear layer, output_fc. We return not only the output but also the second hidden layer as we will do some analysis on it later.

There is no formula to tell us how many layers or how many neurons to have in each layer. The general rule though, is that the layers close to the input have to have higher number of neurons in order to extract general features (e.g. lines, curves, edges), and the number of neurons is decreasing in each next layer, combining  the features learnt by previous layers into more high level features (e.g. the intersection of two lines making a cross, multiple curves make a circle). We want therefore a information compression on each layer


"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable

class MLP(torch.nn.Module):

    def __init__(self):
        super(MLP, self).__init__()
        self.inputl = nn.Linear(256, 180)
        self.hidden1 = nn.Linear(180, 100)
        self.hidden2 = nn.Linear(100, 50)
        self.outputl = nn.Linear(50, 10)
  

    def forward(self, x):
        x = F.relu(self.inputl(x))
        x = F.relu(self.hidden1(x))
        x = F.relu(self.hidden2(x))
        x = self.outputl(x)
        return F.log_softmax(x, dim = 1)

import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable

def train(net, optimizer, criterion, epochs):

  print("Neural Network:",net)
      

  log_interval = 10
  for epoch in range(epochs):
      for batch_idx, (data, target) in enumerate(traindata):
          data, target = Variable(data), Variable(target)
          data = data.float()
          target = target.long()
          data = data.view(-1, 16*16)
          optimizer.zero_grad()
          net_out = net(data)
          loss = criterion(net_out, target)
          loss.backward()
          optimizer.step()
          if batch_idx % log_interval == 0:
              print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                      epoch, batch_idx * len(data), len(traindata.dataset),
                            100. * batch_idx / len(traindata), loss.item()))

"""We made the training func, now lets make the instance and train our model"""

net=MLP()      
optimizer = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.9)
criterion = nn.CrossEntropyLoss()     
epochs = 50


train(net,optimizer,criterion, epochs)

#test on the test dataset


def evaluate(net, criterion):
  with torch.no_grad():
    epoch_loss = 0
    accuracy = 0
    for data, target in testdata:
        data = data.float()
        target = target.long()
        data, target = Variable(data), Variable(target)
        data = data.view(-1, 16 * 16)
        net_out = net(data)

        # add batch loss
        epoch_loss += criterion(net_out, target).item()
        pred = net_out.data.max(1)[1]  # get the index of the max log-probability
        accuracy += pred.eq(target.data).sum()

    epoch_loss /= len(testdata.dataset)
    print('\nTest: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
            epoch_loss, accuracy, len(testdata.dataset),
            100. * accuracy / len(testdata.dataset)))

evaluate(net, criterion)

"""## Experimenting with the learning rate and the epochs:"""

net=MLP()      
optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9)
criterion = nn.CrossEntropyLoss()     
epochs = 50


train(net,optimizer,criterion, epochs)

evaluate(net, criterion)

net=MLP()      
optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
criterion = nn.CrossEntropyLoss()     
epochs = 50


train(net,optimizer,criterion, epochs)

evaluate(net, criterion)

"""Smaller learning rate, still learning but slower"""

net=MLP()      
optimizer = torch.optim.SGD(net.parameters(), lr=0.1)
criterion = nn.CrossEntropyLoss()     
epochs = 15


train(net,optimizer,criterion, epochs)

evaluate(net, criterion)

"""Fewer epochs, the model didn't reach its full potential, but got very close to it (1%)"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable

class MLP2(torch.nn.Module):

    def __init__(self):
        super(MLP2, self).__init__()
        self.inputl = nn.Linear(256, 180)
        self.hidden1 = nn.Linear(180, 100)
        self.hidden2 = nn.Linear(100, 50)
        self.outputl = nn.Linear(50, 10)
  

    def forward(self, x):
        x = F.sigmoid(self.inputl(x))
        x = F.sigmoid(self.hidden1(x))
        x = F.sigmoid(self.hidden2(x))
        x = self.outputl(x)
        return F.log_softmax(x, dim = 1)

net= MLP2()      
optimizer = torch.optim.SGD(net.parameters(), lr=0.1)
criterion = nn.CrossEntropyLoss()     
epochs = 50


train(net,optimizer,criterion, epochs)

"""Implenting our first model replacing the relu activation function for the sigmoid one, even though our model is still learning, its accuracy is seriously deteriorating"""

evaluate(net, criterion)

"""Removing the first hidden layer and implementing again our first model (relu) it is surprisingly close to the first accuracy, being just a bit worse."""

class MLP3(torch.nn.Module):

    def __init__(self):
        super(MLP3, self).__init__()
        self.inputl = nn.Linear(256, 100)
        self.hidden2 = nn.Linear(100, 50)
        self.outputl = nn.Linear(50, 10)
  

    def forward(self, x):
        x = F.relu(self.inputl(x))
        x = F.relu(self.hidden2(x))
        x = self.outputl(x)
        return F.log_softmax(x, dim = 1)

net= MLP3()      
optimizer = torch.optim.SGD(net.parameters(), lr=0.1)
criterion = nn.CrossEntropyLoss()     
epochs = 50


train(net,optimizer,criterion, epochs)

evaluate(net, criterion)



"""### γ)

"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
from torch.utils.data import Dataset, DataLoader
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.utils.validation import check_X_y, check_array, check_is_fitted
from sklearn.utils.multiclass import unique_labels
from sklearn.model_selection import learning_curve
import numpy as np

class ScikitDataset(Dataset):
    """Dataset for scikit-learn package use."""
    def __init__(self, dataset, transform=None):
        """
        Args:
            data_file: np.array of dataset           
            transform (callable, optional): Optional transform to be applied
            on a sample.
        """
        self.dataset = dataset
        self.transform = transform

    def __len__(self):
        # Return the number of samples
        return len(self.dataset)

    def __getitem__(self, idx):
        # Get the sample with index equal to idx
        if torch.is_tensor(idx):
            idx = idx.tolist()
        sample = np.array(self.dataset[idx])
        if self.transform:
            sample = self.transform(sample)
        return sample[1:], sample[0]  


class ScikitDataset_test(Dataset):
    """Dataset for scikit-learn package use."""
    def __init__(self, dataset, transform=None):
        """
        Args:
            data_file: np.array of dataset           
            transform (callable, optional): Optional transform to be applied
            on a sample.
        """
        self.dataset = dataset
        self.transform = transform

    def __len__(self):
        # Return the number of samples
        return len(self.dataset)

    def __getitem__(self, idx):
        # Get the sample with index equal to idx
        if torch.is_tensor(idx):
            idx = idx.tolist()
        sample = np.array(self.dataset[idx])
        if self.transform:
            sample = self.transform(sample)
        return sample     
    

class Net(nn.Module):
    def __init__(self, layers_size, activation_functions):
        self.activation_functions = activation_functions
        super(Net, self).__init__()
        
        layers_len= len(layers_size)
        if len(layers_size) < 3:
            ValueError('#layers must be at least 3')



        self.layers_size=layers_size
        self.lay0=torch.nn.Linear(layers_size[0],layers_size[1])


        for i in range(1,len(layers_size)-1):
          setattr(self,'lay'+str(i),torch.nn.Linear(layers_size[i],layers_size[i+1]))
                
                
    def forward(self, x):
        i = 0
        for af in self.activation_functions:
            x = af(getattr(self,'lay'+str(i)))(x)                 
            i =i+1

        return x     
    

class NeuralNetwork(BaseEstimator, ClassifierMixin):
    # Fully-connected Neural Network compatible with sklearn
    def __init__(self, n_features, n_classes, layers_size=[10,10,10], n_layers=2, lr=0.05, epochs=10):
        '''
            n_features:    number of features
            n_classes:     number of classes
            layers_size:   list of layers sizes
            n_layers:      number of layers (hidden and output)
        '''
        self.net = None   
        self.n_layers = n_layers
        self.layers_size = layers_size
        self.activation_functions = []
        self.n_features = n_features
        self.n_classes = n_classes
        self.lr = lr
        self.criterion = None
        self.optimizer = None
        self.epochs = epochs


        for layer in range(n_layers-1):
            relu = lambda x : F.relu(x)
            self.activation_functions.append(relu)
        log_softmax = lambda x : F.log_softmax(x)
        self.activation_functions.append(log_softmax)   
        
    
    def fit(self, X, y):
        dataAll = np.c_[y, X]
        dataset = ScikitDataset(dataAll)
        train_loader = DataLoader(dataset, batch_size=100,
                    shuffle=False, num_workers=4) 
        
        # NeN 
        self.net = Net(self.layers_size, self.activation_functions)
        
        self.optimizer = torch.optim.SGD(self.net.parameters(), lr=self.lr, momentum=0.9)
        self.criterion = nn.NLLLoss()     
        log_interval = self.epochs / 4
        for epoch in range(self.epochs):
            for batch_idx, (data, target) in enumerate(train_loader):
                data, target = Variable(data), Variable(target)
                data = data.float()
                target = target.long()
                data = data.view(-1, self.n_features)
                self.optimizer.zero_grad()
                net_out = self.net(data)
                loss = self.criterion(net_out, target)
                loss.backward()
                self.optimizer.step()
                if batch_idx % log_interval == 0:
                     print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                             epoch, batch_idx * len(data), len(train_loader.dataset),
                                    100. * batch_idx / len(train_loader), loss.item()))
        return self


    def predict(self, X):
        dataset = ScikitDataset_test(X)
        test_loader = DataLoader(dataset, batch_size=1,
                    shuffle=False, num_workers=4) 
        
        y_predict = []
        for data in test_loader:
            data = data.float()
            data = Variable(data, volatile=True)
            data = data.view(-1, self.n_features)
            net_out = self.net(data)
            y_predict.append(net_out.data.max(1)[1].item())  # get the index of the max log-probability         
            
        return y_predict
    
    
    def score(self, X, y_truth):       
        y_predict = self.predict(X)

net = NeuralNetwork(256, 10, [256,400,100,50,10], 4)
net.fit(X_train, Y_train)
print(net.score(X_test, Y_test))



"""### δ)"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report

# Set the parameters by cross-validation
tuned_parameters = [{'lr': [0.1], 'epochs': [10], 'n_layers': [3], 'layers_size': [[256, 180, 120, 10]]},
                    {'lr': [0.01], 'epochs': [30], 'n_layers': [2], 'layers_size': [[256, 350, 10]]},
                    {'lr': [0.15], 'epochs': [30], 'n_layers': [3], 'layers_size': [[256, 250, 180, 10]]},
                    {'lr': [0.001], 'epochs': [100], 'n_layers': [3], 'layers_size': [[256, 100, 20, 10]]}]

scores = ['accuracy']

for score in scores:
    print("# Tuning hyper-parameters for %s" % score)
    clf = GridSearchCV(NeuralNetwork(n_features=256, n_classes=10), tuned_parameters, cv=5,
                       scoring=score)
    clf.fit(X_train, Y_train)

    print("Best parameters set found on development set:")
    print(clf.best_params_)
    print("Grid scores on development set:")
    means = clf.cv_results_['mean_test_score']
    stds = clf.cv_results_['std_test_score']
    for mean, std, params in zip(means, stds, clf.cv_results_['params']):
        print("%0.3f (+/-%0.03f) for %r"
#               % (mean, std * 2, params))
    y_true, y_predict = Y_test, clf.predict(X_test)
    print(classification_report(y_true, y_predict))
    print()